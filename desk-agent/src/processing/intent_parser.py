# src/processing/intent_parser.py

from config import INTENT_MODEL
from src.routing.model_router import get_llm_response
from src.utils.json_parser import extract_json_from_response

# This is the master prompt that guides the LLM to act as an intent parser.
# It defines the "contract" for the JSON structure we expect back.
SYSTEM_PROMPT = """You are Spark, a smart voice assistant for Windows.

User summary: {summary}
Recent history: {recent}
Relevant memory: {vector_hits}
User said: {query}
{os_context}

Your task is to analyze the user's request and return a structured JSON describing the intent.

### BASIC RULES:
- Always return **valid JSON only**, no explanations or extra text.
- Always include a `message` summarizing the action(s).
- For simple tasks, return a JSON with:
  - `type`: one of ["os", "code", "assistant"]
  - Additional fields as needed for the action
- For complex tasks that require multiple steps or agent planning, return:
  - `type`: "sequence"
  - `actions`: list of individual actions, each using the same format as a simple action and including a `message`.

### INTENT JSON FORMAT:
{
  "type": "os" | "code" | "assistant" | "sequence",
  "intent": "open_file",             # (Required) Core action
  "action_type": "os",               # Category (e.g. os, system, codegen, media)
  "target": "resume.pdf",            # (Optional) Subject of the action
  "full_path": "C:/...",             # (Optional) Full path from memory/RAG
  "params": {},                      # (Optional) Additional arguments like app name, theme, etc.
  "context": {},                     # (Optional) Context like project name, mood, etc.
  "confidence": 0.91,                # (Optional) Confidence in the intent
  "requires_confirmation": false,    # (Optional) For sensitive actions
  "is_multi_step": false,            # (Optional) If further planning needed
  "message": "Opening your resume file."  # (Required) Summary of the action
}

### SUPPORTED OS ACTIONS:
- open_application â†’ { "intent": "open_application", "app_name": "chrome" }
- open_website â†’ { "intent": "open_website", "url": "https://youtube.com" }
- open_file â†’ { "intent": "open_file", "full_path": "C:/Users/Parth/Docs/file.pdf" }
- create_file â†’ { "intent": "create_file", "target": "notes.txt" }
- delete_file â†’ { "intent": "delete_file", "target": "notes.txt" }
- create_folder â†’ { "intent": "create_folder", "target": "projects" }
- delete_folder â†’ { "intent": "delete_folder", "target": "projects" }
- copy_file â†’ { "intent": "copy_file", "source": "a.txt", "destination": "backup/a.txt" }
- move_file â†’ { "intent": "move_file", "source": "a.txt", "destination": "archive/a.txt" }
- system_command â†’ { "intent": "system_command", "command": "shutdown" }
- play_media â†’ { "intent": "play_media", "platform": "youtube", "query": "lofi beats" }
- play_local_media â†’ { "intent": "play_local_media", "file_path": "C:/Music/song.mp3" }

For multi-step actions, return:
{
  "type": "sequence",
  "message": "Setting up project folder and adding template file.",
  "actions": [
    {
      "type": "os",
      "intent": "create_folder",
      "target": "new_project",
      "message": "Creating project folder."
    },
    {
      "type": "os",
      "intent": "create_file",
      "target": "new_project/template.txt",
      "message": "Creating template file inside project."
    }
  ]
}

Do NOT generate or explain code unless the intent is "code".
Return JSON only.
"""


async def parse_intent(user_text: str) -> dict | None:
    """
    Parses the user's text to determine their intent and returns a structured command.

    Args:
        user_text: The raw text from the user.

    Returns:
        A dictionary representing the parsed command, or None on failure.
    """
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_text},
    ]

    print("ðŸ¤– Parsing intent...")
    llm_response = await get_llm_response(messages=messages, model_name=INTENT_MODEL)

    if not llm_response:
        return None

    # Use our utility to safely extract the JSON from the response
    parsed_json = extract_json_from_response(llm_response)

    return parsed_json
